<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>`clustRviz` Computational Details • clustRviz</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.4.0/flatly/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="`clustRviz` Computational Details">
<meta property="og:description" content="clustRviz">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">clustRviz</a>
        <span class="version label label-info" data-toggle="tooltip" data-placement="bottom" title="clustRviz is not yet on CRAN">0.1.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fas fa fas fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/Acknowledgements.html">Acknowledgements</a>
    </li>
    <li>
      <a href="../articles/ClustRVizAlgorithm.html">`clustRviz` Computational Details</a>
    </li>
    <li>
      <a href="../articles/ClustRVizDetails.html">clustRviz Details</a>
    </li>
    <li>
      <a href="../articles/ClustRVizWeights.html">Weight Selection for Convex Clustering and BiClustering</a>
    </li>
    <li>
      <a href="../articles/QuickStart.html">clustRviz Quick Start</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/DataSlingers/clustRviz/">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      

      </header><script src="ClustRVizAlgorithm_files/accessible-code-block-0.0.1/empty-anchor.js"></script><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>
<code>clustRviz</code> Computational Details</h1>
                        <h4 class="author">Michael Weylandt</h4>
            <address class="author_afil">
      Department of Statistics, Rice University<br><a class="author_email" href="mailto:#"></a><a href="mailto:michael.weylandt@rice.edu" class="email">michael.weylandt@rice.edu</a>
      </address>
                              <h4 class="author">John Nagorski</h4>
            <address class="author_afil">
      Department of Statistics, Rice University<br><h4 class="author">Genevera I. Allen</h4>
            <address class="author_afil">
      <div class="line-block">Departments of Statistics, Computer Science, and Electical and Computer Engineering, Rice University<br>
      Jan and Dan Duncan Neurological Research Institute, Baylor College of Medicine</div>
<br><a class="author_email" href="mailto:#"></a><a href="mailto:gallen@rice.edu" class="email">gallen@rice.edu</a>
      </address>
                  
            <h4 class="date">Last Updated: January 8, 2019</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/DataSlingers/clustRviz/blob/master/vignettes/ClustRVizAlgorithm.Rmd"><code>vignettes/ClustRVizAlgorithm.Rmd</code></a></small>
      <div class="hidden name"><code>ClustRVizAlgorithm.Rmd</code></div>

    </address>
</div>

    
    
<p>In this vignette, we give an overview of the <code>CARP</code> and <code>CBASS</code> algorithms. For more details, see Weylandt, Nagorski, and Allen <span class="citation">(2019)</span>.</p>
<div id="convex-clustering" class="section level2">
<h2 class="hasAnchor">
<a href="#convex-clustering" class="anchor"></a>Convex Clustering</h2>
<p><code>CARP</code> begins with the convex clustering problem originally posed by Hocking <em>et al.</em> <span class="citation">(2011)</span>:<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></p>
<p><span class="math display">\[\text{arg min}_{U} \frac{1}{2}\|U - X\|_F^2 + \lambda\sum_{(i, j) \in \mathcal{E}} \|U_{i\cdot} - U_{j\cdot}\|_q\]</span></p>
<p>Note that the second term can be written as <span class="math inline">\(\|DU\|_{q, 1} = \sum_l \|(DU)_{l\cdot}\|_q\)</span> where</p>
<p><span class="math display">\[D_{l\cdot} \text{ is a vector of zeros except having a 1 where edge $l$ starts and a $-1$ where it ends} \]</span></p>
<p>giving the problem</p>
<p><span class="math display">\[\text{arg min}_{U} \frac{1}{2}\|U - X\|_F^2 + \lambda\|DU\|_q\]</span></p>
<p>As noted by Chi and Lange <span class="citation">(2015)</span>, this formulation suggests the use of an operator splitting method. We consider an ADMM algorithm <span class="citation">(Boyd et al. 2011)</span>, beginning by introducing a copy variable <span class="math inline">\(V = DU\)</span> to reformulate the problem as:</p>
<p><span class="math display">\[\text{arg min}_{U} \frac{1}{2}\|U - X\|_F^2 + \lambda\|V\|_{q, 1} \text{ subject to } DU - V = 0\]</span></p>
<p>In our experiments, we have found that working in matrix notation, rather than the vectorized approach of Chi and Lange <span class="citation">(2015)</span>, yields code which is faster as well as more easily maintained.</p>
<p>We then analyze this problem in a matrix analogue of the scaled form ADMM presented in Section 3.1.1 of Boyd <em>et al</em> <span class="citation">(2011)</span>:</p>
<p><span class="math display">\[\begin{align*}
U^{(k + 1)} &amp;= \text{arg min}_U \frac{1}{2}\|U - X\|_F^2 + \frac{\rho}{2}\|DU - V^{(k)} + Z^{(k)}\|_F^2 \\
V^{(k + 1)} &amp;= \text{arg min}_V \lambda\|V\|_{q, 1} + \frac{\rho}{2}\|DU^{(k + 1)} - V + Z^{(k)}\|_F^2 \\
Z^{(k + 1)} &amp;= Z^{(k)} + DU^{(k+1)} - V^{(k+1)}
\end{align*}\]</span></p>
<p>Note that our matrix variables <span class="math inline">\(U, V, Z\)</span> correspond to Boyd <em>et al.</em>’s vector variables <span class="math inline">\(x, z, u\)</span>.</p>
<p>The first problem can be solved exactly by relatively simple algebra. We note that the Frobenius norm terms can be combined to express the problem as <span class="math display">\[\begin{align*}
\text{arg min}_U &amp; \frac{1}{2}\|U - X\|_F^2 + \frac{1}{2}\|\sqrt{\rho} * (DU - V^{(k)} + Z^{(k)})\|_F^2 \\
\text{arg min}_U &amp; \frac{1}{2}\left\|\begin{pmatrix} I \\ \sqrt{\rho}D\end{pmatrix} U - \begin{pmatrix} X \\ \sqrt{\rho}(V^{(k)} - Z^{(k)}) \end{pmatrix}\right\|_F^2
\end{align*}\]</span></p>
<p>This latter term is essentially a multi-response (ridge) regression problem and has an analytical solution given by: <span class="math display">\[\left(\begin{pmatrix} I \\ \sqrt{\rho}D \end{pmatrix}^T\begin{pmatrix} I \\ \sqrt{\rho}D \end{pmatrix}\right)^{-1}\begin{pmatrix} I \\ \sqrt{\rho}D \end{pmatrix}^T\begin{pmatrix} X \\ \sqrt{\rho}(V^{(k)} - Z^{(k)}) \end{pmatrix} = \left(I + \rho D^TD\right)^{-1}\left[X + \rho D^T\left(V^{(k)} - Z^{(k)}\right)\right]\]</span></p>
<p>Next, we note that the <span class="math inline">\(V^{(k)}\)</span> can be expressed in terms of a proximal operator: <span class="math display">\[\text{arg min}_V \lambda \|V\|_{q, 1} + \frac{\rho}{2}\|DU^{(k + 1)} - V + Z^{(k)}\|_F^2 = \textsf{prox}_{\|\cdot\|_{q, 1} * \lambda/\rho}(DU^{(k + 1)} + Z^{(k)})\]</span> where the matrix norm <span class="math inline">\(\|\cdot\|_{q, 1}\)</span> is the sum of the <span class="math inline">\(\ell_q\)</span>-norm of each row. Since this norm is separable across rows, evaluation of the overall proximal operator can be reduced to evaluation of the proximal operator of the <span class="math inline">\(\ell_q\)</span>-norm.</p>
<p><code>clustRviz</code> currently only supports the <span class="math inline">\(q = 1, 2\)</span> cases, which have closed form solutions: <span class="math display">\[V^{(k +1)}_{ij} = \textsf{SoftThresh}_{\lambda/\rho}\left((DU^{(k+1)} + Z^{(k)})_{ij}\right) \text{ when } q = 1\]</span> and <span class="math display">\[V^{(k +1)}_{i\cdot} = \left(1 - \frac{\lambda}{\rho \|(DU^{(k + 1)} + Z^{(k)})_{i\cdot}\|_2}\right)_+(DU^{(k + 1)} + Z^{(k)})_{i\cdot}\text{ when } q = 2\]</span></p>
<p>The <span class="math inline">\(Z^{(k)}\)</span> update is trivial.</p>
<p>The combined algorithm is thus given by: <span class="math display">\[\begin{align*}
U^{(k + 1)} &amp;= (I + \rho D^TD)^{-1}\left[X + \rho D^T*(V^{(k)} - Z^{(k)})\right]\\
V^{(k + 1)} &amp;= \textsf{SoftThresh}_{\lambda / \rho}((DU^{(k + 1)} + Z^{(k)})) \\
Z^{(k + 1)} &amp;= Z^{(k)} + DU^{(k +1)} - V^{(k + 1)}
\end{align*}\]</span> in the <span class="math inline">\(\ell_1\)</span> case and <span class="math display">\[\begin{align*}
U^{(k + 1)} &amp;= (I + \rho D^TD)^{-1}\left[X + \rho D^T*(V^{(k)} - Z^{(k)})\right]\\
V^{(k + 1)}_{i\cdot} &amp;= \left(1 - \frac{\lambda}{\rho \|(DU^{(k + 1)} + Z^{(k)})_{i\cdot}\|_2}\right)_+(DU^{(k + 1)} + Z^{(k)})_{i\cdot} \qquad \text{ for each } i \\
Z^{(k + 1)} &amp;= Z^{(k)} + DU^{(k +1)} - V^{(k + 1)}
\end{align*}\]</span> in the <span class="math inline">\(\ell_2\)</span> case.</p>
<p>In practice, we pre-compute a Cholesky factorization of <span class="math inline">\(I + \rho D^TD\)</span> which can be used in each <span class="math inline">\(U\)</span> update.</p>
<p>We use these updates in an algorithmic regularization scheme, as described in Hu, Chi, and Allen <span class="citation">(2016)</span> to obtain the standard (non-backtracking) <code>CARP</code> algorithm:</p>
<ul>
<li>Input:
<ul>
<li>Data Matrix: <span class="math inline">\(X \in \mathbb{R}^{n \times p}\)</span>
</li>
<li>Weighted edge set: <span class="math inline">\(\mathcal{E} = \{(e_l, w_l)\}\)</span>
</li>
<li>Relaxation Parameter: <span class="math inline">\(\rho \in \mathbb{R}_{&gt; 0}\)</span>
</li>
</ul>
</li>
<li>Precompute:
<ul>
<li>Difference matrix <span class="math inline">\(D \in \mathbb{R}^{|\mathcal{E}| \times n}\)</span>
</li>
<li>Cholesky factor <span class="math inline">\(L = \textsf{chol}(I + \rho D^TD) \in \mathbb{R}^{n \times n}\)</span>
</li>
</ul>
</li>
<li>Initialize:
<ul>
<li>
<span class="math inline">\(U^{(0)} = X\)</span>, <span class="math inline">\(V^{(0)} = DX\)</span>, <span class="math inline">\(Z^{(0)} = V^{(0)}\)</span>, <span class="math inline">\(\gamma^{(1)} = \epsilon\)</span>, <span class="math inline">\(k = 1\)</span>
</li>
</ul>
</li>
<li>Repeat until <span class="math inline">\(\|V^{(k - 1)}\| = 0\)</span>
<ul>
<li><span class="math inline">\(U^{(k)} = L^{-T}L^{-1}\left[X + \rho D^T(V^{(k - 1)} - Z^{(k - 1)})\right]\)</span></li>
<li>If <span class="math inline">\(q = 1\)</span>, for all <span class="math inline">\((i, j)\)</span>: <span class="math display">\[V_{ij}^{(k)} = \textsf{SoftThreshold}_{w_i \gamma^{(k)}/ \rho}((DU^{(k)} + Z^{(k - 1)})_{ij})\]</span>
</li>
<li>If <span class="math inline">\(q = 2\)</span>, for all <span class="math inline">\(l\)</span>: <span class="math display">\[V^{(k)}_{l\cdot} = \left(1 - \frac{\gamma^{(k)} w_l}{\rho\|(DU^{(k)} + Z^{(k - 1)})_{l\cdot}\|_2}\right)_+(DU^{(k)} + Z^{(k - 1)})_{l\cdot}\]</span>
</li>
<li><span class="math inline">\(Z^{(k)} = Z^{(k - 1)} + DU^{(k)} - V^{(k)}\)</span></li>
<li><span class="math inline">\(\gamma^{(k + 1)} = t \gamma^{(k)}\)</span></li>
<li><span class="math inline">\(k := k + 1\)</span></li>
</ul>
</li>
<li>Return <span class="math inline">\(\{(U^{(l)}, V^{(l)}\}_{l = 0}^{k - 1}\)</span>
</li>
</ul>
<p>In <code>clustRviz</code>, we do not return the <span class="math inline">\(Z^{(k)}\)</span> iterates, but we do return the <span class="math inline">\(U^{(k)}\)</span> and <span class="math inline">\(V^{(k)}\)</span> iterates, as well as the zero pattern of the latter (which is useful for identifying clusters and forming dendrograms).</p>
</div>
<div id="convex-bi-clustering" class="section level2">
<h2 class="hasAnchor">
<a href="#convex-bi-clustering" class="anchor"></a>Convex Bi-Clustering</h2>
<p><code>CBASS</code> begins with the convex biclustering problem originally posed by Chi, Allen, and Baraniuk <span class="citation">(2017)</span>:<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p><span class="math display">\[\text{arg min}_{U} \frac{1}{2}\|U - X\|_F^2 + \lambda\left(\sum_{(i, j) \in \mathcal{E_1}} \|U_{i\cdot} - U_{j\cdot}\|_q + \sum_{(k, l) \in \mathcal{E_2}}\|U_{\cdot k} - U_{\cdot l}\|_q\right)\]</span></p>
<p>As before, we simplify notation by introducing two difference matrices <span class="math inline">\(D_{\text{row}}, D_{\text{col}}\)</span> to write the problem as:</p>
<p><span class="math display">\[\text{arg min}_{U} \frac{1}{2}\|U - X\|_F^2 + \lambda\left(\|D_{\text{row}}U\|_{q, 1} + \|UD_{\text{col}}\|_{1, q}\right)\]</span></p>
<p>We recognize this as the proximal operator of the function <span class="math inline">\(f(U) = \|D_{\text{row}}U\|_{q, 1} + \|UD_{\text{col}}\|_{1, q}\)</span>. Despite the simplicity of the proximal operators of the individual terms, the proximal operator of the sum cannot be computed explicitly. To address this difficulty, we use the Dykstra-Like Proximal Algorithm (DLPA) of Bauschke and Combettes <span class="citation">(2008; see also Combettes and Pesquet 2011)</span> which allows us to evaluate the proximal operator of the sum by repeated evaluation of the proximal operators of the summands.</p>
<p>DLPA works by repeating the following iterates until convergence:</p>
<ul>
<li><span class="math inline">\(T = \textsf{prox}_{\lambda \|D_{\text{row}}\cdot\|_{q, 1}}(U^{(n)} + P^{(n)})\)</span></li>
<li><span class="math inline">\(P^{(n + 1)} = P^{(n)} + U^{(n)} - T\)</span></li>
<li><span class="math inline">\(U^{(n + 1)} = \textsf{prox}_{\lambda \|\cdot D_{\text{col}}\|_{1, q}}(T + Q^{(n)})\)</span></li>
<li><span class="math inline">\(Q^{(n + 1)} = Q^{(n)} + T - U^{(n + 1)}\)</span></li>
</ul>
<p>where we initialize <span class="math inline">\(U^{(0)} = X\)</span> and <span class="math inline">\(P^{(0)} = Q^{(0)} = 0\)</span>.</p>
<p>The reader may consider <span class="math inline">\(T\)</span> as an intermediate <span class="math inline">\(U\)</span>-iterate and denote it as <span class="math inline">\(T = U^{(n + 1/2)}\)</span> to make its role more clear.</p>
<p>We note that the two proximal operators are non-trivial and require use of an iterative algorithm at each evaluation. Thankfully, we have already addressed these problems. The first proximal operator can be written as:</p>
<p><span class="math display">\[\text{arg min}_X \frac{1}{2}\|X - (U^{(n)} + P^{(n)})\|_F^2 + \lambda\|D_{\text{row}}X\|_{q, 1}\]</span></p>
<p>This is exactly the form of convex clustering, with <span class="math inline">\(X\)</span> serving as the free variable and <span class="math inline">\(U^{(n)} + P^{(n)}\)</span> playing the role of the data. Similarly, the second proximal operator can be written as</p>
<p><span class="math display">\[\text{arg min}_X \frac{1}{2}\|X - (T + Q^{(n)})\|_F^2 + \lambda\|XD_{\text{col}}\|_{1, q}\]</span></p>
<p>This is not quite the problem previously considered, but by transposing everything, noting the invariance of the Frobenius norm under transposition and the duality of the <span class="math inline">\(\|\cdot\|_{q, 1}\)</span> and <span class="math inline">\(\|\cdot\|_{1, q}\)</span> norms under transposition, we see that this problem is equivalent to:</p>
<p><span class="math display">\[\text{arg min}_{X^T} \frac{1}{2}\|X^T - (T + Q^{(n)})^T\|_F^2 + \lambda\|D_{\text{col}}^TX^T\|_{q, 1} = \textsf{prox}_{\|D_{\text{col}}^T\cdot\|_{q, 1}}\left[(T + Q^{(n)})^T\right]\]</span></p>
<p>which is convex clustering of <span class="math inline">\((T + Q^{(n)})^T\)</span> with the difference matrix <span class="math inline">\(D_{\text{col}}^T\)</span>. Note also that, since we are minimizing over <span class="math inline">\(X^T\)</span>, we are principally interested in the transpose of the value of the proximal operator. Putting this together, we have the DLPA updates:</p>
<ul>
<li><span class="math inline">\(T = \textsf{prox}_{\lambda \|D_{\text{row}}\cdot\|_{q, 1}}(U^{(n)} + P^{(n)})\)</span></li>
<li><span class="math inline">\(P^{(n + 1)} = P^{(n)} + U^{(n)} - T\)</span></li>
<li><span class="math inline">\(U^{(n + 1)} = (\textsf{prox}_{\lambda \|D_{\text{col}}^T\cdot\|_{q, 1}}\left[(T + Q^{(n)})^T\right])^T\)</span></li>
<li><span class="math inline">\(Q^{(n + 1)} = Q^{(n)} + T - U^{(n + 1)}\)</span></li>
</ul>
<p>In the <code>CBASS</code> context, we use an operating splitting scheme to deal with the complexity of the <span class="math inline">\(\|A\cdot\|_{q, 1}\)</span>-norm proximal operators. In particular, we use a single ADMM step, rather than solving the subproblems to convergence, yielding the <code>CBASS</code> iterates:</p>
<ul>
<li>
<span class="math inline">\(T = (I + \rho D_{\text{row}}^TD_{\text{row}})^{-1}\left[U^{(n)} + P^{(n)} + \rho D_{\text{row}}^T\left(V_{\text{row}}^{(n)} - Z_{\text{row}}^{(n)}\right)\right]\)</span> (Row ADMM Primal Update)</li>
<li>
<span class="math inline">\(V_{\text{row}}^{(n+1)} = \textsf{prox}_{\lambda / \rho\|\cdot\|_{q, 1}}(D_{\text{row}}T + Z_{\text{row}}^{(n)})\)</span> (Row ADMM Copy Update)</li>
<li>
<span class="math inline">\(Z_{\text{row}}^{(n+1)} = Z^{(n)}_{\text{row}} + D_{\text{row}}T - V^{(n+1)}_{\text{row}}\)</span> (Row ADMM Dual Update)</li>
<li><span class="math inline">\(P^{(n + 1)} = P^{(n)} + U^{(n)} - T\)</span></li>
<li>
<span class="math inline">\(S = (I + \rho D_{\text{col}}D_{\text{col}}^T)^{-1}\left[(T + Q^{(n)})^T + \rho D_{\text{col}}\left(V_{\text{col}}^{(n)} - Z_{\text{col}}^{(n)}\right)\right]\)</span> (Column ADMM Primal Update)</li>
<li>
<span class="math inline">\(V_{\text{col}}^{(n+1)} = \textsf{prox}_{\lambda / \rho\|\cdot\|_{q, 1}}(D_{\text{col}}^TS + Z_{\text{col}}^{(n)})\)</span> (Column ADMM Copy Update)</li>
<li>
<span class="math inline">\(Z_{\text{col}}^{(n+1)} = Z^{(n)}_{\text{col}} + D_{\text{col}}^TS - V^{(n+1)}_{\text{col}}\)</span> (Column ADMM Dual Update)</li>
<li><span class="math inline">\(U^{(n + 1)} = S^T\)</span></li>
<li><span class="math inline">\(Q^{(n + 1)} = Q^{(n)} + T - U^{(n + 1)}\)</span></li>
</ul>
<p>In practice, we can obtain speed-ups by caching Cholesky factorizations of <span class="math inline">\((I + \rho D_{\text{row}}^TD_{\text{row}})\)</span> and <span class="math inline">\((I + \rho D_{\text{col}}D_{\text{col}}^T)\)</span> for repeated use.</p>
<p>Using these updates in an algorthmic regularization scheme <span class="citation">(Hu, Chi, and Allen 2016)</span>, we obtain the standard (non-backtracking) <code>CBASS</code> algorithm:</p>
<ul>
<li>Input:
<ul>
<li>Data Matrix: <span class="math inline">\(X \in \mathbb{R}^{n \times p}\)</span>
</li>
<li>Weighted edge sets: <span class="math inline">\(\mathcal{E}_{\text{row}} = \{(e_l, w_l)\}\)</span> and <span class="math inline">\(\mathcal{E}_{\text{column}} = \{(e_l, w_l)\}\)</span>
</li>
<li>Relaxation Parameter: <span class="math inline">\(\rho \in \mathbb{R}_{&gt; 0}\)</span>
</li>
</ul>
</li>
<li>Precompute:
<ul>
<li>Row difference matrix <span class="math inline">\(D_{\text{row}} \in \mathbb{R}^{|\mathcal{E}_{\text{row}}| \times n}\)</span>
</li>
<li>Column difference matrix <span class="math inline">\(D_{\text{col}} \in \mathbb{R}^{p \times |\mathcal{E}_{\text{col}}|}\)</span>
</li>
<li>Row Cholesky factor <span class="math inline">\(L_{\text{row}} = \textsf{chol}(I + \rho D_{\text{row}}^TD_{\text{row}}) \in \mathbb{R}^{n \times n}\)</span>
</li>
<li>Column Cholesky factor <span class="math inline">\(L_{\text{col}} = \textsf{chol}(I + \rho D_{\text{col}}D_{\text{col}}^T) \in \mathbb{R}^{p \times p}\)</span>
</li>
</ul>
</li>
<li>Initialize:
<ul>
<li>
<span class="math inline">\(U^{(0)} = X\)</span>, <span class="math inline">\(V^{(0)}_{\text{row}} = D_{\text{row}}X\)</span>, <span class="math inline">\(Z^{(0)}_{\text{row}} = V^{(0)}_{\text{row}}\)</span>, <span class="math inline">\(V^{(0)}_{\text{col}} = (XD_{\text{col}})^T = D_{\text{col}}^TX^T\)</span>, <span class="math inline">\(Z^{(0)}_{\text{col}} = V^{(0)}_{\text{col}}\)</span>, <span class="math inline">\(P^{(0)} = Q^{(0)} = 0\)</span>, <span class="math inline">\(\gamma^{(1)} = \epsilon\)</span>, <span class="math inline">\(k = 0\)</span>
</li>
</ul>
</li>
<li>Repeat until <span class="math inline">\(\|V^{(k - 1)}_{\text{row}}\| = \|V^{(k - 1)}_{\text{col}}\| = 0\)</span>
<ul>
<li>Row Updates:
<ul>
<li><span class="math inline">\(T = L^{-T}_{\text{row}}L^{-1}_{\text{row}}\left[U^{(k)} + P^{(k)} + \rho D^T_{\text{row}}(V^{(k - 1)}_{\text{row}} - Z^{(k - 1)}_{\text{row}})\right]\)</span></li>
<li>If <span class="math inline">\(q = 1\)</span>, for all <span class="math inline">\((i, j)\)</span>: <span class="math display">\[(V^{(k)}_{\text{row}})_{ij} = \textsf{SoftThreshold}_{w^{\text{row}}_i \gamma^{(k)}/ \rho}((D_{\text{row}}T + Z^{(k - 1)}_{\text{row}})_{ij})\]</span>
</li>
<li>If <span class="math inline">\(q = 2\)</span>, for all <span class="math inline">\(l\)</span>: <span class="math display">\[(V^{(k)}_{\text{row}})_{l\cdot} = \left(1 - \frac{\gamma^{(k)} w_l}{\rho\|(D_{\text{row}}T + Z^{(k - 1)}_{\text{row}})_{l\cdot}\|_2}\right)_+(D_{\text{row}}T + Z^{(k - 1)}_{\text{row}})_{l\cdot}\]</span>
</li>
<li><span class="math inline">\(Z^{(k)}_{\text{row}} = Z^{(k - 1)}_{\text{row}} + D_{\text{row}}T - V^{(k)}_{\text{row}}\)</span></li>
</ul>
</li>
<li><span class="math inline">\(P^{(k)} = P^{(k - 1)} + U^{(k - 1)} - T\)</span></li>
<li>Column Updates:
<ul>
<li><span class="math inline">\(S = L^{-T}_{\text{col}}L^{-1}_{\text{col}}\left[(T + Q^{(k)})^T + \rho D_{\text{col}}(V^{(k - 1)}_{\text{col}} - Z^{(k - 1)}_{\text{col}})\right]\)</span></li>
<li>If <span class="math inline">\(q = 1\)</span>, for all <span class="math inline">\((i, j)\)</span>: <span class="math display">\[(V^{(k)}_{\text{col}})_{ij} = \textsf{SoftThreshold}_{w^{\text{col}}_i \gamma^{(k)}/ \rho}((D_{\text{col}}^TS + Z^{(k - 1)}_{\text{col}})_{ij})\]</span>
</li>
<li>If <span class="math inline">\(q = 2\)</span>, for all <span class="math inline">\(l\)</span>: <span class="math display">\[(V^{(k)}_{\text{col}})_{l\cdot} = \left(1 - \frac{\gamma^{(k)} w_l}{\rho\|(D_{\text{col}}^TS + Z^{(k - 1)}_{\text{col}})_{l\cdot}\|_2}\right)_+(D_{\text{col}}^TS + Z^{(k - 1)}_{\text{col}})_{l\cdot}\]</span>
</li>
<li><span class="math inline">\(Z^{(k)}_{\text{col}} = Z^{(k - 1)}_{\text{col}} + D_{\text{col}}^TS - V^{(k)}_{\text{col}}\)</span></li>
</ul>
</li>
<li><span class="math inline">\(U^{(k)} = S^T\)</span></li>
<li><span class="math inline">\(Q^{(k)} = Q^{(k - 1)} + T - U^{(k)}\)</span></li>
<li><span class="math inline">\(\gamma^{(k + 1)} = t \gamma^{(k)}\)</span></li>
<li><span class="math inline">\(k := k + 1\)</span></li>
</ul>
</li>
<li>Return <span class="math inline">\(\{(U^{(l)}, V^{(l)}_{\text{row}}, V^{(l)}_{\text{col}})\}_{l = 0}^{k - 1}\)</span>
</li>
</ul>
<p>Note that, unlike in the <code>COBRA</code> algorithm of Chi <em>et al.</em> <span class="citation">(2017)</span> or the DLPA on which it is based <span class="citation">(Bauschke and Combettes 2008)</span>, we keep the auxiliary ADMM variables <span class="math inline">\(V_{\text{row}}, Z_{\text{row}}, V_{\text{col}}, Z_{\text{col}}\)</span> from one iteration to the next, rather than starting each sub-problem <em>de novo</em>.</p>
</div>
<div id="references" class="section level2 unnumbered">
<h2 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h2>
<div id="refs" class="references">
<div id="ref-Bauschke:2008">
<p>Bauschke, Heinz H., and Patrick L. Combettes. 2008. “A Dykstra-Like Algorithm for Two Monotone Operators.” <em>Pacific Journal of Optimization</em> 4 (3): 383–91.</p>
</div>
<div id="ref-Boyd:2011">
<p>Boyd, Stephen, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. 2011. “Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers.” <em>Foundations and Trends in Machine Learning</em> 3 (1): 1–122. <a href="https://doi.org/10.1561/2200000016">https://doi.org/10.1561/2200000016</a>.</p>
</div>
<div id="ref-Chi:2017">
<p>Chi, Eric C., Genevera I. Allen, and Richard G. Baraniuk. 2017. “Convex Biclustering.” <em>Biometrics</em> 73 (1): 10–19. <a href="https://doi.org/10.1111/biom.12540">https://doi.org/10.1111/biom.12540</a>.</p>
</div>
<div id="ref-Chi:2015">
<p>Chi, Eric C., and Kenneth Lange. 2015. “Splitting Methods for Convex Clustering.” <em>Journal of Computational and Graphical Statistics</em> 24 (4): 994–1013. <a href="https://doi.org/10.1080/10618600.2014.948181">https://doi.org/10.1080/10618600.2014.948181</a>.</p>
</div>
<div id="ref-Combettes:2011">
<p>Combettes, Patrick L., and Jean-Cristophe Pesquet. 2011. “Proximal Splitting Methods in Signal Processing.” In <em>Fixed-Point Algorithms for Inverse Problems in Science and Engineering</em>, edited by Heinz H. Bauschke, Regina S. Burachik, Patrick L. Combettes, Veit Elser, D. Russell Luke, and Henry Wolkowicz, 185–212. Springer. <a href="https://doi.org/10.1007/978-1-4419-9569-8_10">https://doi.org/10.1007/978-1-4419-9569-8_10</a>.</p>
</div>
<div id="ref-Hocking:2011">
<p>Hocking, Toby Dylan, Armand Joulin, Francis Bach, and Jean-Philippe Vert. 2011. “Clusterpath: An Algorithm for Clustering Using Convex Fusion Penalties.” In <em>ICML 2011: Proceedings of the 28th International Conference on Machine Learning</em>, edited by Lise Getoor and Tobias Scheffer, 745–52. Bellevue, Washington, USA: ACM. <a href="http://www.icml-2011.org/papers/419_icmlpaper.pdf">http://www.icml-2011.org/papers/419_icmlpaper.pdf</a>.</p>
</div>
<div id="ref-Hu:2016">
<p>Hu, Yue, Eric C. Chi, and Genevera I. Allen. 2016. “ADMM Algorithmic Regularization Paths for Sparse Statistical Machine Learning.” In <em>Splitting Methods in Communication and Imaging, Science, and Engineering</em>, edited by Roland Glowinski, Stanley J. Osher, and Wotao Yin, 433–49. Springer. <a href="https://doi.org/10.1007/978-3-319-41589-5_13">https://doi.org/10.1007/978-3-319-41589-5_13</a>.</p>
</div>
<div id="ref-Weylandt:2019">
<p>Weylandt, Michael, John Nagorski, and Genevera I. Allen. 2019. “Dynamic Visualization and Fast Computation for Convex Clustering via Algorithmic Regularization.” <em>ArXiv Pre-Print 1901.01477</em>. <a href="https://arxiv.org/abs/1901.01477">https://arxiv.org/abs/1901.01477</a>.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr>
<ol>
<li id="fn1"><p>Here, we consider the case of uniform weights to simplify some of the notation, but the general case is essentially the same. The general formulation of <code>CARP</code> is given below.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Again, we consider the case of uniform weights to simplify some of the notation and give the general case at the end of this section.<a href="#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

        <nav id="toc" data-toggle="toc"><h2 data-toc-skip>Contents</h2>
    </nav>
</div>

</div>



      <footer><div class="copyright">
  <p>Developed by Michael Weylandt, John Nagorski, Genevera Allen, Daniel Englund.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.5.1.</p>
</div>

      </footer>
</div>

  


  </body>
</html>
